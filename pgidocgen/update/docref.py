# Copyright 2015,2017 Christoph Reiter
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.

import json
from urllib.parse import unquote

import requests
from bs4 import BeautifulSoup
from multiprocessing.pool import ThreadPool

from ..girdata import get_docref_path
from ..girdata.library import LIBRARIES
from ..util import progress


def add_parser(subparsers):
    parser = subparsers.add_parser(
        "update-docref", help="Update the doc references")
    parser.set_defaults(func=main)


def fetch_pages(lib):
    pages = set()
    keywords = set()
    r = requests.get(lib.devhelp_url)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")

    for tag in soup.findAll("sub"):
        page = tag["link"]
        if page.startswith(("index-", "api-index-",
                            "annotation-glossary", "ix")):
            continue
        if "#" in page:
            continue
        pages.add(page)

    for tag in soup.findAll("keyword"):
        if tag.get("link"):
            if tag["link"].startswith('gdbus-'):
                continue
            keywords.add(tag["link"])

    return pages, keywords


def fetch_page(arg):
    lib, page, keywords = arg

    names = {}
    r = requests.get(lib.url + page)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")
    for link in soup.findAll("a"):
        if not link.get("name"):
            continue
        if "." in link["name"]:
            # Mostly, links containing dots are noise. Except for links
            # generated by gdbus-codegen, which look like that:
            #     <link linkend="gdbus-method-%s.%s">
            if not link["name"].startswith('gdbus-'):
                continue
        url = page + "#" + link["name"]
        if url not in keywords:
            names[unquote(link["name"])] = lib.url + url
    return names, page


def main(args):
    with ThreadPool(30) as pool:
        for lib in LIBRARIES:
            print("Update %s.." % lib.namespace)
            pages, keywords = fetch_pages(lib)
            mapping = {}
            with progress(len(pages)) as update:
                for i, (names, page) in enumerate(pool.imap_unordered(
                        fetch_page, [(lib, p, keywords) for p in pages])):
                    update(i + 1)
                    mapping.update(names)

            ns = lib.namespace
            namespace, version = ns.split("-")
            with open(get_docref_path(namespace, version), "wb") as h:
                h.write(
                    json.dumps(mapping, sort_keys=True, indent=4).encode("utf-8"))
